{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d09743",
   "metadata": {},
   "source": [
    "## Gemini PDF Processing and Validation Script  \n",
    "#### Automated extraction of mining resource tables using Google Gemini API\n",
    "\n",
    "---\n",
    "\n",
    "#### Overview\n",
    "This script uses **Googleâ€™s Gemini API** to extract **structured mineral resource data** from filtered PDF pages (e.g., those produced by the Unstructured processing stage).  \n",
    "It prompts Gemini to output clean, pipe-delimited (`|`) text files that adhere to a predefined schema for **mineral resource estimation (MRE)** data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Core Functions\n",
    "##### `validate_output_file(file_path)`\n",
    "Validates the generated `.txt` output for data integrity and schema compliance:\n",
    "- Ensures correct **column count and order**\n",
    "- Checks valid **deposit_type** and **mineral_resource** values\n",
    "- Confirms that numeric fields (`tonnage_kt`, `grade_gt`) contain valid numbers\n",
    "\n",
    "##### `process_pdfs_with_gemini(input_folder, output_folder, max_retries, retry_delay)`\n",
    "Coordinates the main Gemini-based extraction process:\n",
    "1. Initializes the Gemini client using your `GEMINI_API_KEY`  \n",
    "2. Iterates through all PDFs in the input folder  \n",
    "3. Sends each file (plus an extraction prompt) to Gemini for structured output  \n",
    "4. Saves the `.txt` output and runs validation  \n",
    "5. Retries failed extractions (up to `max_retries`) before moving invalid results to a `failed_validation` folder  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“‚ Expected Input and Output\n",
    "**Input folder:**  \n",
    "`unstructured/pdf_filtered/` â€” filtered PDFs from the Unstructured pipeline  \n",
    "\n",
    "**Output folder:**  \n",
    "`unstructured/gemini/normalized_output/` â€” validated `.txt` outputs  \n",
    "\n",
    "**Failed validation folder:**  \n",
    "`unstructured/gemini/normalized_output/failed_validation/`  \n",
    "\n",
    "---\n",
    "\n",
    "#### Output Format\n",
    "Each line in the output `.txt` file follows this **pipe-delimited schema**:\n",
    "\n",
    "| Column | Description |\n",
    "|:--|:--|\n",
    "| `project_name` | Name of the mining project |\n",
    "| `deposit_name` | Deposit or orebody name |\n",
    "| `deposit_type` | One of: open-pit, underground, heap, stockpile, other, unknown |\n",
    "| `date_updated` | Report date (ISO format preferred) |\n",
    "| `mineral_resource` | One of: inferred, indicated, measured, total |\n",
    "| `cutoff_gt` | Cutoff grade (grams per tonne) |\n",
    "| `mineral` | Primary mineral (symbol) |\n",
    "| `mineral_list` | List of minerals (e.g., Au-Cu-Zn) |\n",
    "| `tonnage_kt` | Ore tonnage (kilotonnes) |\n",
    "| `grade_gt` | Average grade (grams per tonne) |\n",
    "\n",
    "---\n",
    "\n",
    "#### Workflow Summary\n",
    "1. Define validation schema (`EXPECTED_COLUMNS`) and valid categories  \n",
    "2. Send filtered PDFs to Gemini for extraction  \n",
    "3. Validate each generated output  \n",
    "4. Retry or move invalid outputs to `failed_validation`  \n",
    "5. Produce clean, structured text ready for data ingestion  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”‘ Environment Variable\n",
    "Ensure your **Gemini API key** is set before running:\n",
    "```bash\n",
    "export GEMINI_API_KEY=\"your_api_key_here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cabe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "EXPECTED_COLUMNS = [\n",
    "    'project_name', 'deposit_name', 'deposit_type', 'date_updated',\n",
    "    'mineral_resource', 'cutoff_gt', 'mineral', 'mineral_list',\n",
    "    'tonnage_kt', 'grade_gt'\n",
    "]\n",
    "\n",
    "VALID_DEPOSIT_TYPES = ['open-pit', 'underground', 'heap', 'stockpile', 'other', 'unknown']\n",
    "VALID_RESOURCE_TYPES = ['inferred', 'indicated', 'measured', 'total']\n",
    "\n",
    "def validate_output_file(file_path):\n",
    "    \"\"\"\n",
    "    Validate the TXT output for data quality.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line.split('|')\n",
    "\n",
    "        # Check column count\n",
    "        if len(parts) != len(EXPECTED_COLUMNS):\n",
    "            return False, f\"Invalid column count on line {i+1}: {len(parts)}\"\n",
    "\n",
    "        # Skip header line if it matches expected columns\n",
    "        if i == 0 and set(parts) == set(EXPECTED_COLUMNS):\n",
    "            continue\n",
    "\n",
    "        project_name, deposit_name, deposit_type, date_updated, mineral_resource, cutoff_gt, mineral, mineral_list, tonnage_kt, grade_gt = parts\n",
    "\n",
    "        # Check deposit_type validity\n",
    "        if deposit_type.lower() not in VALID_DEPOSIT_TYPES and deposit_type != '':\n",
    "            return False, f\"Invalid deposit_type on line {i+1}: {deposit_type}\"\n",
    "\n",
    "        # Check mineral_resource validity\n",
    "        if mineral_resource.lower() not in VALID_RESOURCE_TYPES and mineral_resource != '':\n",
    "            return False, f\"Invalid mineral_resource on line {i+1}: {mineral_resource}\"\n",
    "\n",
    "        # Check numeric values\n",
    "        if tonnage_kt and not is_numeric(tonnage_kt):\n",
    "            return False, f\"Non-numeric tonnage_kt on line {i+1}: {tonnage_kt}\"\n",
    "\n",
    "        if grade_gt and not is_numeric(grade_gt):\n",
    "            return False, f\"Non-numeric grade_gt on line {i+1}: {grade_gt}\"\n",
    "\n",
    "    return True, \"Validation passed\"\n",
    "\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def process_pdfs_with_gemini(input_folder, output_folder, max_retries, retry_delay):\n",
    "    try:\n",
    "        # Initialize Gemini client\n",
    "        client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "        # Create output and failed directories if they don't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        failed_folder = os.path.join(output_folder, 'failed_validation')\n",
    "        os.makedirs(failed_folder, exist_ok=True)\n",
    "\n",
    "        # Define the prompt\n",
    "        prompt = (\n",
    "            \"You are a data extraction agent specialized in mining reports.\"\n",
    "            \"The following is a subset of pages containing tables from a mining report PDF.\"\n",
    "            \"From the document, output txt (use | delimiter) with the following columns: \"\n",
    "            \"'project_name', 'deposit_name', 'deposit_type', 'date_updated', 'mineral_resource', 'cutoff_gt', 'mineral', 'mineral_list',\"\n",
    "            \"'tonnage_kt', and 'grade_gt'.\"\n",
    "            \"Only extract data related to mineral resources, skip from ore reserves table (skip prove/probable).\"\n",
    "            \"deposit_type is 'open-pit', 'underground', 'heap', 'stockpile', 'other', 'unknown'.\"\n",
    "            \"mineral_resource is 'inferred', 'indicated', 'measured', or 'total'.\"\n",
    "            \"cutoff is primarily for gold, unknown if not specified.\"\n",
    "            \"mineral is atomic symbol like ag,au,bi,co,cu,fe,in,mo,pb,pd,pt,re,s,sb,sg,ta2o5,wo3,zn.\"\n",
    "            \"mineral list is list of mineral symbol separated by dash -.\"\n",
    "            \"Convert tonnage to kt kiloton if needed. Convert grade to gt grams per tonne if needed.\"\n",
    "            \"If multiple date_updated exists for a project, deposit, and deposit_type, use only data from the latest date.\"\n",
    "            \"Stick to the predefined column values for deposit_type and mineral_resource, do not use any other variations.\"\n",
    "            \"Do not include subtotals or totals from the tables, unless they are the only values present.\"\n",
    "            \"Output empty values if none found. Stick to the column structure, do not include any other text or explanation, just the CSV output.\"\n",
    "        )\n",
    "\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                file_path = pathlib.Path(os.path.join(input_folder, filename))\n",
    "                output_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "                output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "                if os.path.exists(output_path):\n",
    "                    print(f\"Skipping {filename} (output already exists: {output_path})\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                retries = 0\n",
    "\n",
    "                while retries < max_retries:\n",
    "                    try:\n",
    "                        # Upload the PDF using the File API\n",
    "                        sample_file = client.files.upload(file=file_path)\n",
    "\n",
    "                        # Generate content from the model\n",
    "                        response = client.models.generate_content(\n",
    "                            model=\"gemini-2.5-flash\",\n",
    "                            contents=[sample_file, prompt]\n",
    "                        )\n",
    "\n",
    "                        # Save the response to a text file\n",
    "                        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(response.text)\n",
    "                        print(f\"Saved output to: {output_path}\")\n",
    "\n",
    "                        # Validate the output\n",
    "                        valid, message = validate_output_file(output_path)\n",
    "                        if valid:\n",
    "                            print(\"Validation passed âœ…\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Validation failed: {message}\")\n",
    "                            # Delete invalid file\n",
    "                            os.remove(output_path)\n",
    "                            retries += 1\n",
    "                            if retries < max_retries:\n",
    "                                print(f\"Retrying extraction (Attempt {retries}/{max_retries}) in {retry_delay} seconds...\")\n",
    "                                time.sleep(retry_delay)\n",
    "                            else:\n",
    "                                print(f\"Max retries reached for {filename}. Moving to failed folder.\")\n",
    "                                failed_path = os.path.join(failed_folder, output_filename)\n",
    "                                with open(failed_path, 'w', encoding='utf-8') as f:\n",
    "                                    f.write(f\"FAILED VALIDATION after {max_retries} attempts.\\nLast error: {message}\\n\")\n",
    "                                print(f\"Moved failed log to: {failed_path}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {filename}: {e}\")\n",
    "                        if \"503\" in str(e):\n",
    "                            retries += 1\n",
    "                            print(f\"Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n",
    "                            time.sleep(retry_delay)\n",
    "                        else:\n",
    "                            print(\"Skipping retries for a non-transient error.\")\n",
    "                            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = 'unstructured/pdf_filtered'\n",
    "    output_folder = 'unstructured/gemini/normalized_output'\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    \n",
    "    process_pdfs_with_gemini(input_folder, output_folder, max_retries, retry_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d08668",
   "metadata": {},
   "source": [
    "### Data Cleaning and Consolidation Script  \n",
    "### Combine and validate Gemini outputs for structured MRE dataset\n",
    "\n",
    "---\n",
    "\n",
    "#### Overview\n",
    "This script cleans, normalizes, and merges all Gemini-extracted `.txt` files into a single Excel file.  \n",
    "It ensures consistent column structure, valid date formats, and removes unnecessary total/overall rows for **gold (Au)** deposits.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Steps\n",
    "1. **Read and validate** all `.txt` files from  \n",
    "   `unstructured/gemini/normalized_output/`\n",
    "2. **Normalize dates** using flexible parsing (handles `year`, `month-year`, or full dates)  \n",
    "3. **Combine** all valid rows into a single DataFrame  \n",
    "4. **Add metadata columns**:\n",
    "   - `filename` â€” source file name  \n",
    "   - `key` â€” unique composite key  \n",
    "   - `number_of_mineral` â€” count of listed minerals  \n",
    "5. **Filter Au dataset**  \n",
    "   - Removes `total`, `overall`, `subtotal` entries if detailed resources exist  \n",
    "6. **Save output** to:\n",
    "   - `Combined` â€” all valid rows  \n",
    "   - `Filtered_Au` â€” cleaned gold data only  \n",
    "   - `Log` â€” records errors or mismatched rows  \n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "ðŸ“ **File:** `combined_output.xlsx`  \n",
    "Contains three sheets:  \n",
    "- **Combined** â†’ all processed data  \n",
    "- **Filtered_Au** â†’ cleaned gold subset  \n",
    "- **Log** â†’ issues or encoding errors  \n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Notes\n",
    "- Handles inconsistent encodings gracefully  \n",
    "- Ensures date normalization with fallback for incomplete values  \n",
    "- Designed for **Gemini MRE outputs** with a 10-column expected schema  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac50209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from calendar import monthrange\n",
    "import numpy as np\n",
    "\n",
    "# === CONFIG ===\n",
    "input_folder = \"unstructured/gemini/normalized_output\"\n",
    "output_file = \"combined_output.xlsx\"\n",
    "\n",
    "# Expected header\n",
    "expected_header = [\n",
    "    \"project_name\", \"deposit_name\", \"deposit_type\", \"date_updated\",\n",
    "    \"mineral_resource\", \"cutoff_gt\", \"mineral\", \"mineral_list\", \"tonnage_kt\", \"grade_gt\"\n",
    "]\n",
    "expected_col_count = len(expected_header)\n",
    "\n",
    "\n",
    "# === FUNCTIONS ===\n",
    "def normalize_date(date_str):\n",
    "    if not date_str or date_str.strip().lower() in [\"unknown\", \"n/a\", \"none\"]:\n",
    "        return None  # Return None for missing values\n",
    "    \n",
    "    # Split only on \" and \" or \";\", NOT comma\n",
    "    parts = re.split(r'\\s+and\\s+|;', date_str.strip(), flags=re.IGNORECASE)\n",
    "    date_candidates = []\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        \n",
    "        # Skip incomplete date fragments (must contain year)\n",
    "        if not re.search(r'\\d{4}', part):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            parsed_date = parser.parse(part, dayfirst=True, fuzzy=True)\n",
    "            date_candidates.append(parsed_date)\n",
    "        except:\n",
    "            # Handle year only\n",
    "            if re.fullmatch(r'\\d{4}', part):\n",
    "                y = int(part)\n",
    "                date_candidates.append(datetime(y, 12, 31))\n",
    "            elif re.match(r'^[A-Za-z]{3,9}[- ]?\\d{4}$', part):  # Month Year\n",
    "                try:\n",
    "                    temp = parser.parse(part, fuzzy=True)\n",
    "                    last_day = monthrange(temp.year, temp.month)[1]\n",
    "                    date_candidates.append(datetime(temp.year, temp.month, last_day))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    if not date_candidates:\n",
    "        return None\n",
    "    \n",
    "    return max(date_candidates)  # Return datetime object\n",
    "\n",
    "\n",
    "# === MAIN PROCESS ===\n",
    "all_rows = []\n",
    "error_log = []\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.lower().endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            error_log.append(f\"File: {file_name} - Encoding issue\")\n",
    "            continue\n",
    "        \n",
    "        for idx, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            parts = [p.strip() for p in line.split(\"|\")]\n",
    "            \n",
    "            # Remove header row if present\n",
    "            if idx == 0 and [p.lower() for p in parts] == [h.lower() for h in expected_header]:\n",
    "                continue\n",
    "            \n",
    "            # Validate column count\n",
    "            if len(parts) != expected_col_count:\n",
    "                error_log.append(f\"File: {file_name}, Row: {idx+1}, Columns: {len(parts)} (expected {expected_col_count})\")\n",
    "                continue\n",
    "            \n",
    "            # Normalize date (4th column)\n",
    "            parts[3] = normalize_date(parts[3])\n",
    "            \n",
    "            all_rows.append(parts + [file_name])\n",
    "\n",
    "# Add filename to header\n",
    "output_header = expected_header + [\"filename\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_rows, columns=output_header)\n",
    "\n",
    "# Convert date_updated to datetime and allow NaT\n",
    "df[\"date_updated\"] = pd.to_datetime(df[\"date_updated\"], errors=\"coerce\")\n",
    "\n",
    "# === Add extra columns ===\n",
    "df[\"key\"] = df[\"project_name\"].str.lower().str.strip() + \"|\" + \\\n",
    "            df[\"deposit_name\"].str.lower().str.strip() + \"|\" + \\\n",
    "            df[\"deposit_type\"].str.lower().str.strip() + \"|\" + \\\n",
    "            df[\"date_updated\"].astype(str) + \"|\" + \\\n",
    "            df[\"mineral\"].str.lower().str.strip()\n",
    "\n",
    "df[\"number_of_mineral\"] = df[\"mineral_list\"].apply(lambda x: len(x.split(\"-\")) if isinstance(x, str) and x.strip() else 0)\n",
    "\n",
    "# === Filter for Au FIRST ===\n",
    "df_au = df[df[\"mineral\"].str.lower() == \"au\"].copy()\n",
    "\n",
    "# === Remove rows with 'total' or 'overall' in deposit_name for Au data only ===\n",
    "# Create a grouping key for Au data (without date_updated to group related deposits)\n",
    "df_au[\"grouping_key\"] = df_au[\"project_name\"].str.lower().str.strip() + \"|\" + \\\n",
    "                        df_au[\"deposit_name\"].str.lower().str.strip() + \"|\" + \\\n",
    "                        df_au[\"deposit_type\"].str.lower().str.strip()\n",
    "\n",
    "# Identify rows with 'total' or 'overall' in deposit_name (including sub-total, subtotal, etc.)\n",
    "has_total_overall = df_au[\"deposit_name\"].str.lower().str.contains(\"total|overall|sub-total|subtotal\", na=False)\n",
    "\n",
    "# Group by the grouping_key to check for detailed resources\n",
    "rows_to_keep = []\n",
    "\n",
    "for grouping_key, group in df_au.groupby(\"grouping_key\"):\n",
    "    # Check if this group has any measured/indicated/inferred mineral resources\n",
    "    has_detailed_resources = group[\"mineral_resource\"].str.lower().isin([\"inferred\", \"indicated\", \"measured\"]).any()\n",
    "    \n",
    "    if has_detailed_resources:\n",
    "        # If group has detailed resources anywhere, remove total/overall rows\n",
    "        filtered_group = group[~has_total_overall.loc[group.index]]\n",
    "        print(f\"Removed total/overall from Au group: {grouping_key[:50]}...\")\n",
    "    else:\n",
    "        # If group doesn't have any detailed resources, keep all rows (including total/overall)\n",
    "        filtered_group = group\n",
    "        print(f\"Kept all rows for Au group: {grouping_key[:50]}...\")\n",
    "    \n",
    "    rows_to_keep.append(filtered_group)\n",
    "\n",
    "# Combine all filtered Au groups\n",
    "df_au = pd.concat(rows_to_keep, ignore_index=True)\n",
    "\n",
    "print(f\"Au dataset has {len(df_au)} rows\")\n",
    "print(f\"Au rows with total/overall remaining: {df_au['deposit_name'].str.lower().str.contains('total|overall', na=False).sum()}\")\n",
    "\n",
    "# === Save to Excel ===\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\", date_format=\"YYYY-MM-DD\") as writer:\n",
    "    df.to_excel(writer, index=False, sheet_name=\"Combined\")\n",
    "    df_au.to_excel(writer, index=False, sheet_name=\"Filtered_Au\")\n",
    "    \n",
    "    log_df = pd.DataFrame(error_log if error_log else [\"No errors\"], columns=[\"Log\"])\n",
    "    log_df.to_excel(writer, index=False, sheet_name=\"Log\")\n",
    "\n",
    "print(f\"âœ… Combined and filtered files saved to {output_file}\")\n",
    "if error_log:\n",
    "    print(\"âš  Some rows had issues. Check 'Log' sheet in the Excel file.\")\n",
    "else:\n",
    "    print(\"âœ… All rows processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928e991",
   "metadata": {},
   "source": [
    "### MRE Wide-Format Transformation Script  \n",
    "#### Convert long-format dataset into wide-format with grade, tonnage, and gold value\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "This script reshapes the prepared MRE dataset by:\n",
    "- Pivoting **grade** and **tonnage** by resource category  \n",
    "- Calculating **total tonnage** and **weighted average grade** when missing  \n",
    "- Computing **gold values (oz)** for each resource type  \n",
    "- Exporting the final wide-format dataset to Excel  \n",
    "\n",
    "**Input:** `prepared_dataset.xlsx`  \n",
    "**Output:** `prepared_dataset_wide_mre_with_goldval.xlsx`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cef0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === CONFIG ===\n",
    "input_file = \"prepared_dataset.xlsx\"  # Replace with your file\n",
    "output_file = \"prepared_dataset_wide_mre_with_goldval.xlsx\"\n",
    "\n",
    "# === READ DATA ===\n",
    "df = pd.read_excel(input_file)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# === NORMALIZE mineral_resource ===\n",
    "df['mineral_resource'] = df['mineral_resource'].str.strip().str.lower()\n",
    "\n",
    "# === CREATE GRADE WIDE COLUMNS ===\n",
    "grade_wide = df.pivot_table(\n",
    "    index=[\"asx_code\", \"company_name\", \"project_name\", \"deposit_name\", \"deposit_type\"],\n",
    "    columns=\"mineral_resource\",\n",
    "    values=\"grade_gt\",\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "grade_wide.columns.name = None\n",
    "grade_wide = grade_wide.rename(columns={\n",
    "    \"indicated\": \"grade_indicated_gt\",\n",
    "    \"measured\": \"grade_measured_gt\",\n",
    "    \"inferred\": \"grade_inferred_gt\",\n",
    "    \"total\": \"grade_total_gt\"\n",
    "})\n",
    "\n",
    "# === CREATE TONNAGE WIDE COLUMNS ===\n",
    "tonnage_wide = df.pivot_table(\n",
    "    index=[\"asx_code\", \"company_name\", \"project_name\", \"deposit_name\", \"deposit_type\"],\n",
    "    columns=\"mineral_resource\",\n",
    "    values=\"tonnage_kt\",\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "tonnage_wide.columns.name = None\n",
    "tonnage_wide = tonnage_wide.rename(columns={\n",
    "    \"indicated\": \"tonnage_indicated_kt\",\n",
    "    \"measured\": \"tonnage_measured_kt\",\n",
    "    \"inferred\": \"tonnage_inferred_kt\",\n",
    "    \"total\": \"tonnage_total_kt\"\n",
    "})\n",
    "\n",
    "# === MERGE WIDE DATA BACK WITH OTHER COLUMNS ===\n",
    "desc_cols = [\"filename\", \"announcement_date\", \"date_updated\", \"loc_country\", \"loc_state\",\n",
    "             \"loc_district\", \"loc_city\", \"loc_coordinates\", \"loc_notes\",\n",
    "             \"project_status\", \"source_of_funding\", \"funding_notes\", \"funding_party\"]\n",
    "\n",
    "desc_df = df.groupby([\"asx_code\", \"company_name\", \"project_name\", \"deposit_name\", \"deposit_type\"])[desc_cols].first().reset_index()\n",
    "\n",
    "wide_df = grade_wide.merge(tonnage_wide, on=[\"asx_code\", \"company_name\", \"project_name\", \"deposit_name\", \"deposit_type\"])\n",
    "wide_df = wide_df.merge(desc_df, on=[\"asx_code\", \"company_name\", \"project_name\", \"deposit_name\", \"deposit_type\"])\n",
    "\n",
    "# === FILL total_kt if missing ===\n",
    "kt_cols = [\"tonnage_indicated_kt\", \"tonnage_measured_kt\", \"tonnage_inferred_kt\"]\n",
    "wide_df[\"tonnage_total_kt\"] = wide_df[\"tonnage_total_kt\"].fillna(wide_df[kt_cols].sum(axis=1, skipna=True))\n",
    "\n",
    "# === FILL total_gt if missing (weighted average) ===\n",
    "def weighted_average(row):\n",
    "    if pd.notna(row[\"grade_total_gt\"]):\n",
    "        return row[\"grade_total_gt\"]\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for gt_col, kt_col in zip([\"grade_indicated_gt\",\"grade_measured_gt\",\"grade_inferred_gt\"], \n",
    "                              [\"tonnage_indicated_kt\",\"tonnage_measured_kt\",\"tonnage_inferred_kt\"]):\n",
    "        if pd.notna(row[gt_col]) and pd.notna(row[kt_col]):\n",
    "            num += row[gt_col] * row[kt_col]\n",
    "            denom += row[kt_col]\n",
    "    if denom > 0:\n",
    "        return num / denom\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "wide_df[\"grade_total_gt\"] = wide_df.apply(weighted_average, axis=1)\n",
    "\n",
    "# === COMPUTE GOLD VALUE (in ounces) ===\n",
    "for res in [\"indicated\", \"measured\", \"inferred\", \"total\"]:\n",
    "    grade_col = f\"grade_{res}_gt\"\n",
    "    tonnage_col = f\"tonnage_{res}_kt\"\n",
    "    goldval_col = f\"goldval_{res}_oz\"\n",
    "    wide_df[goldval_col] = wide_df[grade_col] * wide_df[tonnage_col] * 32.1507  # kt * g/t -> oz\n",
    "\n",
    "# === WRITE TO EXCEL ===\n",
    "wide_df.to_excel(output_file, index=False)\n",
    "print(f\"Wide-format MRE data with gold value columns written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IOG_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
