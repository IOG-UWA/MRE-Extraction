{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52ce32d",
   "metadata": {},
   "source": [
    "## ðŸª¨ PDF Processing Script using **Unstructured** Library  \n",
    "### Filter pages containing mining resource keywords and tables\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§­ Overview\n",
    "This notebook automates the extraction of relevant pages from mining-related PDF documents (e.g., ASX announcements, JORC or MRE reports).  \n",
    "It uses the **Unstructured** library to identify and filter pages that contain:\n",
    "- **Main resource keywords** â€” â€œMREâ€, â€œmineral resourceâ€, â€œore reserveâ€  \n",
    "- **Table indicators** â€” â€œindicâ€, â€œmeasâ€, â€œinferâ€, â€œproveâ€, â€œprobâ€  \n",
    "- **Mineral codes/names** â€” â€œAuâ€, â€œCuâ€, â€œFeâ€, â€œgoldâ€, â€œlithiumâ€, etc.  \n",
    "\n",
    "Filtered pages are then exported as both `.json` and `.txt` files, and a summary report is saved as a `.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Key Features\n",
    "- ðŸ” **Keyword and table detection** using Unstructuredâ€™s high-resolution PDF parsing  \n",
    "- ðŸ§© **Page-level filtering** based on combined text and structural features  \n",
    "- ðŸ’¾ **Output formats**: JSON (structured), TXT (readable), CSV (summary)  \n",
    "- ðŸ“ **Automatic directory setup** for input PDFs and processed results  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¦ Requirements\n",
    "Install these packages before running:\n",
    "```bash\n",
    "pip install unstructured pypdfium2 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Processing Script using Unstructured Library\n",
    "# Filter pages containing mining resource keywords and tables   \n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "\n",
    "# Define filtering keywords\n",
    "KEYWORDS = [\"mre\", \"mineral resource\", \"ore reserve\"]\n",
    "TABLE_KEYWORDS = [\"indic\", \"meas\", \"infer\", \"prove\", \"prob\"]\n",
    "MINERAL_CODES = [\"Au\", \"Ag\", \"Pt\", \"Pd\", \"Cu\", \"Pb\", \"Zn\", \"Ni\", \"Co\",\n",
    "                 \"Fe\", \"Mn\", \"Al\", \"Li\", \"Sn\", \"W\", \"Mo\", \"Cr\", \"U\", \"V\", \"REE\"]\n",
    "MINERAL_NAMES = [\"gold\", \"silver\", \"platinum\", \"palladium\", \"copper\", \"lead\", \"zinc\",\n",
    "                 \"nickel\", \"cobalt\", \"iron\", \"manganese\", \"aluminium\", \"lithium\", \"tin\",\n",
    "                 \"tungsten\", \"molybdenum\", \"chromium\", \"uranium\", \"vanadium\", \"rare earth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
    "    input_dir = Path(\"pdf_downloads\")\n",
    "    output_dir = Path(\"unstructured/filtered_pages\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return input_dir, output_dir\n",
    "\n",
    "def contains_keywords(text: str, keywords: List[str], case_sensitive: bool = False) -> bool:\n",
    "    \"\"\"Check if text contains any of the specified keywords.\"\"\"\n",
    "    if not case_sensitive:\n",
    "        text = text.lower()\n",
    "        keywords = [kw.lower() for kw in keywords]\n",
    "    \n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "def has_table_content(elements: List[Any]) -> bool:\n",
    "    \"\"\"Check if elements contain table-related content.\"\"\"\n",
    "    table_elements = [elem for elem in elements if hasattr(elem, 'category') and elem.category == 'Table']\n",
    "    \n",
    "    if not table_elements:\n",
    "        return False\n",
    "    \n",
    "    # Check if table content contains table keywords\n",
    "    for table_elem in table_elements:\n",
    "        table_text = str(table_elem)\n",
    "        if contains_keywords(table_text, TABLE_KEYWORDS):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_page_content(elements: List[Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Filter page based on all criteria:\n",
    "    1. Contains main keywords (mre, mineral resource, ore reserve)\n",
    "    2. Contains table keywords (indic, meas, infer, prove, prob)\n",
    "    3. Contains mineral codes or names\n",
    "    \"\"\"\n",
    "    # Combine all text from the page\n",
    "    page_text = \" \".join([str(elem) for elem in elements])\n",
    "    \n",
    "    # Check main keywords\n",
    "    has_main_keywords = contains_keywords(page_text, KEYWORDS)\n",
    "    \n",
    "    # Check table keywords\n",
    "    has_table_keywords = contains_keywords(page_text, TABLE_KEYWORDS)\n",
    "    \n",
    "    # Check mineral codes or names\n",
    "    has_mineral_codes = contains_keywords(page_text, MINERAL_CODES)\n",
    "    has_mineral_names = contains_keywords(page_text, MINERAL_NAMES)\n",
    "    has_minerals = has_mineral_codes or has_mineral_names\n",
    "    \n",
    "    # All criteria must be met\n",
    "    return has_main_keywords and has_table_keywords and has_minerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_file(pdf_path: Path, output_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single PDF file and extract filtered pages.\"\"\"\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "\n",
    "    try:\n",
    "        # Partition PDF with page-level extraction\n",
    "        elements = partition_pdf(\n",
    "            filename=str(pdf_path),\n",
    "            strategy=\"hi_res\",   # Use \"hi_res\" if OCR is needed\n",
    "            languages=[\"eng\"],\n",
    "            include_metadata=True,\n",
    "            infer_table_structure=True,\n",
    "            extract_images_in_pdf=False,\n",
    "            include_page_breaks=True,\n",
    "        )\n",
    "\n",
    "        # Group elements by page\n",
    "        pages = {}\n",
    "        for element in elements:\n",
    "            page_num = getattr(element.metadata, \"page_number\", None)\n",
    "            if page_num is None:\n",
    "                page_num = -1  # fallback if no page number found\n",
    "\n",
    "            if page_num not in pages:\n",
    "                pages[page_num] = []\n",
    "            pages[page_num].append(element)\n",
    "\n",
    "        # Filter pages\n",
    "        filtered_pages = {}\n",
    "        for page_num, page_elements in pages.items():\n",
    "            if filter_page_content(page_elements):  # <- make sure you defined this\n",
    "                filtered_pages[page_num] = page_elements\n",
    "                print(f\"  âœ“ Page {page_num} matches criteria\")\n",
    "\n",
    "        # Save filtered pages\n",
    "        if filtered_pages:\n",
    "            output_file = output_dir / f\"{pdf_path.stem}_filtered.json\"\n",
    "\n",
    "            def safe_text(elem):\n",
    "                \"\"\"Return safe text representation of an element.\"\"\"\n",
    "                if getattr(elem, \"text\", None):\n",
    "                    return elem.text\n",
    "                try:\n",
    "                    return str(elem) or \"\"\n",
    "                except Exception:\n",
    "                    return \"\"\n",
    "\n",
    "            def safe_metadata(elem):\n",
    "                \"\"\"Return safe metadata dict.\"\"\"\n",
    "                meta = getattr(elem, \"metadata\", None)\n",
    "                if meta is None:\n",
    "                    return {}\n",
    "                to_dict = getattr(meta, \"to_dict\", None)\n",
    "                if callable(to_dict):\n",
    "                    try:\n",
    "                        return to_dict()\n",
    "                    except Exception:\n",
    "                        return {}\n",
    "                return {}\n",
    "\n",
    "            json_data = {\n",
    "                \"source_file\": pdf_path.name,\n",
    "                \"filtered_pages\": {},\n",
    "                \"summary\": {\n",
    "                    \"total_pages\": len(pages),\n",
    "                    \"filtered_pages\": len(filtered_pages),\n",
    "                    \"filtered_page_numbers\": list(filtered_pages.keys()),\n",
    "                },\n",
    "            }\n",
    "\n",
    "            for page_num, page_elements in filtered_pages.items():\n",
    "                json_data[\"filtered_pages\"][str(page_num)] = [\n",
    "                    {\n",
    "                        \"text\": safe_text(elem),\n",
    "                        \"category\": getattr(elem, \"category\", \"Unknown\"),\n",
    "                        \"metadata\": safe_metadata(elem),\n",
    "                    }\n",
    "                    for elem in page_elements\n",
    "                ]\n",
    "\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            # Also save as plain text\n",
    "            text_file = output_dir / f\"{pdf_path.stem}_filtered.txt\"\n",
    "            with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Source: {pdf_path.name}\\n\")\n",
    "                f.write(f\"Filtered Pages: {list(filtered_pages.keys())}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "                for page_num in sorted(filtered_pages.keys()):\n",
    "                    f.write(f\"PAGE {page_num}\\n\")\n",
    "                    f.write(\"-\" * 20 + \"\\n\")\n",
    "                    for elem in filtered_pages[page_num]:\n",
    "                        f.write(f\"{safe_text(elem)}\\n\\n\")\n",
    "                    f.write(\"\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        return {\n",
    "            \"file\": pdf_path.name,\n",
    "            \"total_pages\": len(pages),\n",
    "            \"filtered_pages\": len(filtered_pages),\n",
    "            \"filtered_page_numbers\": list(filtered_pages.keys()),\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error processing {pdf_path.name}: {str(e)}\")\n",
    "        return {\n",
    "            \"file\": pdf_path.name,\n",
    "            \"total_pages\": 0,\n",
    "            \"filtered_pages\": 0,\n",
    "            \"filtered_page_numbers\": [],\n",
    "            \"status\": f\"error: {str(e)}\",\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946133d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setup directories\n",
    "    input_dir, output_dir = setup_directories()\n",
    "\n",
    "    # Check input directory exists\n",
    "    if not input_dir.exists():\n",
    "        print(f\"Input directory '{input_dir}' does not exist. Please add PDF files.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Find all PDFs\n",
    "    pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{input_dir}'\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process.\\n\")\n",
    "\n",
    "    # Process PDFs one by one\n",
    "    results = []\n",
    "    for pdf_file in pdf_files:\n",
    "        result = process_pdf_file(pdf_file, output_dir)\n",
    "        results.append(result)\n",
    "\n",
    "    # Print summary\n",
    "    total_files = len(results)\n",
    "    successful_files = len([r for r in results if r[\"status\"] == \"success\"])\n",
    "    total_filtered_pages = sum(r[\"filtered_pages\"] for r in results)\n",
    "\n",
    "    print(\"\\nProcessing Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Successful: {successful_files}\")\n",
    "    print(f\"Failed: {total_files - successful_files}\")\n",
    "    print(f\"Total filtered pages: {total_filtered_pages}\")\n",
    "\n",
    "    # Save summary CSV\n",
    "    df_results = pd.DataFrame(results)\n",
    "    summary_file = output_dir / \"processing_summary.csv\"\n",
    "    df_results.to_csv(summary_file, index=False)\n",
    "    print(f\"\\nSummary saved to: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IOG_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
